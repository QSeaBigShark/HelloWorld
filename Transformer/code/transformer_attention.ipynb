{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Optional, Any, Union, Callable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**参考**  \n",
    "[知乎:self-attention](https://zhuanlan.zhihu.com/p/455399791)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nn.ModuleList**  \n",
    "nn.ModuleList是pytorch中的一个容器，允许将多个模块组合在一起，形成一个更大的模块。\n",
    "特点：  \n",
    "1. **动态构建模块列表**：可以在创建后，随时动态添加模块到列表中，对于需要在运行时根据条件构建不同子模块的情况非常有用。  \n",
    "2. **不会自动跟踪模块**：nn.ModuleList不会自动跟踪其包含的模块，意味着它的参数不会自动添加到父模块的参数列表中。需要自己手动管理这些参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (1): Linear(in_features=20, out_features=30, bias=True)\n",
      "  (2): Linear(in_features=30, out_features=40, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=10, out_features=20, bias=True)\n",
       "    (1): Linear(in_features=20, out_features=30, bias=True)\n",
       "    (2): Linear(in_features=30, out_features=40, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        #在构造函数中初始化模型\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        #创建一个神经网络层列表，每一个元素都是一个线性层(nn.Linear)\n",
    "        #layer_sizes 是一个包含每个层的输入和输出维度的列表\n",
    "        #循环遍历layer_sizes，创建并添加线性层到layers\n",
    "        self.layers = nn.ModuleList([nn.Linear(layer_sizes[i], layer_sizes[i+1]) for i in range(len(layer_sizes)-1)])\n",
    "        print(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #定义前向传播函数forward\n",
    "        #输入x表示模型的输入数据\n",
    "\n",
    "        #遍历 self.layers中的线性层，依次将输入x传递给每一层\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        #返回最终输出，x包含了前向传播后的模型输出\n",
    "        return x\n",
    "\n",
    "mlist = CustomModel([10, 20, 30, 40])\n",
    "mlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zip argument #1 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1756\\765875904.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: zip argument #1 must support iteration"
     ]
    }
   ],
   "source": [
    "q,k,v = Tensor(), Tensor(), Tensor()\n",
    "mm = [q,k,v]\n",
    "zip(m, mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_heads: int,  #训练多头个数\n",
    "                 d_model: int,     #输入token的embedding维数\n",
    "                 dropout: float = 0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be a multiple of num_heads\"\n",
    "        # Assume v_dim always equals k_dim\n",
    "        self.k_dim = d_model // num_heads #每一个W的维度数  \n",
    "        self.num_heads = num_heads  #训练的attention的多头数\n",
    "        #W^Q, W^K, W^V, W^O，其维度(d_model, d_k))\n",
    "        self.proj_weights = clones(nn.Linear(d_model, d_model), 4) \n",
    "        print(\"###########\")\n",
    "        print(self.proj_weights)\n",
    "        self.attention_score = None #attention_score\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                query: Tensor,\n",
    "                key: Tensor,\n",
    "                value: Tensor,\n",
    "                mask: Optional[Tensor] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: shape (batch_size, seq_len, d_model)\n",
    "            key: shape (batch_size, seq_len, d_model)\n",
    "            value: shape (batch_size, seq_len, d_model)\n",
    "            mask: shape (batch_size, seq_len, seq_len).Since we assume all data use a same mask, so\n",
    "                here the shape also equals to (1, seq_len, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            out: shape (batch_size, seq_len, d_model).The output of a multihead attention layer\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all heads\n",
    "            mask = mask.unsqueeze(1)   #unsqueeze() 增加张量维数，接受参数表示在哪个维度之前增加新维度\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "\n",
    "        print(query)\n",
    "        print(key)\n",
    "        print(value)\n",
    "        m = zip(self.proj_weights, [query, key, value])\n",
    "        print(m)\n",
    "\n",
    "        # 1) Apply W^Q, W^K, W^V to generate new query, key, value\n",
    "        query, key, value\\\n",
    "            = [proj_weight(x).view(batch_size, -1, self.num_heads, self.k_dim).transpose(1,2)\n",
    "               for proj_weight, x in zip(self.proj_weights, [query, key, value])]\n",
    "        print(\"------------------------\")\n",
    "        \n",
    "        # 2) Calculate attention score and the out\n",
    "        out, self.aatention_score = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        \n",
    "        # 3)Concat output\n",
    "        out = out.transpose(1,2).contiguous()\\\n",
    "            .view(batch_size, -1, self.num_heads * self.k_dim)\n",
    "        \n",
    "        # 4) Apply W^o to get the final output\n",
    "        out = self.proj_weights[-1](out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "#定义attention的计算逻辑\n",
    "def attention(query:Tensor,\n",
    "              key:Tensor,\n",
    "              value:Tensor, \n",
    "              mask:Optional[Tensor] = None,\n",
    "              dropout:Optional[Callable] = None):\n",
    "    \"\"\"\n",
    "    Define how to calculate attention score\n",
    "    Args:\n",
    "        query: shape (batch_size, num_heads, seq_len, k_dim)\n",
    "        key: shape (batch_size, num_heads, seq_len, k_dim)\n",
    "        value: shape (batch_size, num_heads, seq_len, v_dim)\n",
    "        mask: shape (batch_size, num_heads, seq_len, seq_len). Since our assumption, here the shape is\n",
    "        (1, 1, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    #取出w^Q, w^K, w^V的维度，以便后续计算attention score时，需要scaled by sqrt(k_dim)\n",
    "    k_dim = query.size(-1)\n",
    "\n",
    "    #shape (seq_len, seq_len), row:token, col:that token's attention score\n",
    "    #transpose(x, y) x,y转置\n",
    "    #Q*K^T,\n",
    "    #query shape(batch_size, num_heads, seq_len, k_dim) \n",
    "    #key^T shape(batch_size, num_heads, k_dim, seq_len)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(k_dim)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    attention_score = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        attention_score = dropout(attention_score)  \n",
    "\n",
    "    out = torch.matmul(attention_score, value)\n",
    "\n",
    "    return out, attention_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (1): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (2): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (3): Linear(in_features=8, out_features=8, bias=True)\n",
      ")\n",
      "<zip object at 0x00000247863D6CC8>\n",
      "------------------------\n",
      "torch.Size([6, 3, 8])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Test MultiHeadedAttention\n",
    "    batch_size = 6\n",
    "    seq_len = 3\n",
    "    d_model = 8\n",
    "    num_heads = 2\n",
    "    #mask = None\n",
    "    mask = torch.tril(torch.ones((seq_len, seq_len)), diagonal = 0).unsqueeze(0)\n",
    "    \n",
    "\n",
    "    input = torch.rand(batch_size, seq_len, d_model)\n",
    "    multi_attn = MultiHeadedAttention(num_heads = num_heads, d_model = d_model, dropout = 0.1)\n",
    "    out = multi_attn(query = input, key = input, value = input, mask = mask)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
